{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrtmFa8IAtps"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUn1Z--HIHSc",
        "outputId": "6b09eb2b-7188-40cc-d934-dce6d0a4a869"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import os\n",
        "import csv\n",
        "import string\n",
        "import librosa\n",
        "import shutil\n",
        "import torch\n",
        "import random\n",
        "import math\n",
        "import h5py\n",
        "import torchaudio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchaudio.transforms as T\n",
        "import torch.optim as optim\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.utils.data.sampler import BatchSampler\n",
        "from torch.utils.data import Subset\n",
        "from librosa.util import normalize\n",
        "from librosa.util import fix_length\n",
        "from google.colab import drive\n",
        "from google.colab import runtime\n",
        "from itertools import combinations\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
        "from torchvision import models, transforms\n",
        "from torchvision.transforms import v2\n",
        "from torchsummary import summary\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision import transforms\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrAl3KiDATum"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHrmA-6ouybO"
      },
      "source": [
        "MNIST (Testing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TWGQaxV6uuit"
      },
      "outputs": [],
      "source": [
        "# mean, std = 0.1307, 0.3081\n",
        "\n",
        "# # Define transforms\n",
        "# transform = v2.Compose([\n",
        "#     v2.Resize((64, 64)),\n",
        "#     v2.ToImage(),\n",
        "#     v2.ToDtype(torch.float32, scale=True),\n",
        "#     v2.Normalize((mean,), (std,))\n",
        "# ])\n",
        "\n",
        "# # Load datasets\n",
        "# train_dataset = MNIST('../data/MNIST', train=True, download=True, transform=transform)\n",
        "# test_dataset = MNIST('../data/MNIST', train=False, download=True, transform=transform)\n",
        "\n",
        "# # Define the target number of samples\n",
        "# num_train_samples = 6400\n",
        "# num_val_samples = 1600\n",
        "\n",
        "# # Shuffle and select a subset of the training dataset\n",
        "# train_indices = np.random.permutation(len(train_dataset))[:num_train_samples]\n",
        "# train_subset = Subset(train_dataset, train_indices)\n",
        "\n",
        "# # Shuffle and select a subset of the validation dataset\n",
        "# val_indices = np.random.permutation(len(test_dataset))[:num_val_samples]\n",
        "# val_subset = Subset(test_dataset, val_indices)\n",
        "\n",
        "# # Create DataLoaders for the subsets\n",
        "# kwargs = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else {}\n",
        "# train_loader = DataLoader(train_subset, shuffle=True, batch_size=256, **kwargs)\n",
        "# val_loader = DataLoader(val_subset, batch_size=256, **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-mIqsO2IHSi"
      },
      "source": [
        "Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DpPQXzhIl0tO"
      },
      "outputs": [],
      "source": [
        "# Load training data from HDF5 file\n",
        "with h5py.File('/content/drive/My Drive/Projects/NeuraBeat/Data/train_data_melspec_expanded.h5', 'r') as train_file:\n",
        "    train_data = np.array(train_file['data'])\n",
        "    train_labels = np.array(train_file['labels'])\n",
        "\n",
        "# Load validation data from HDF5 file\n",
        "with h5py.File('/content/drive/My Drive/Projects/NeuraBeat/Data/val_data_melspec_expanded.h5', 'r') as val_file:\n",
        "    val_data = np.array(val_file['data'])\n",
        "    val_labels = np.array(val_file['labels'])\n",
        "\n",
        "train_data = np.transpose(train_data, (0, 2, 3, 1))\n",
        "val_data = np.transpose(val_data, (0, 2, 3, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "z-EzqmMyLoSa"
      },
      "outputs": [],
      "source": [
        "# plt.figure(figsize=(6, 6))\n",
        "# plt.title('Before Normalization')\n",
        "# print(train_data[0].shape)\n",
        "# plt.imshow(train_data[0])\n",
        "# plt.colorbar()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SAhb0BTj4ovl"
      },
      "outputs": [],
      "source": [
        "def get_mean_std(data):\n",
        "  mean = np.mean(data, axis=(0, 1, 2))\n",
        "  std = np.std(data, axis=(0, 1, 2))\n",
        "  return mean, std\n",
        "\n",
        "mean, std = get_mean_std(train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58erhwFnGr0y"
      },
      "source": [
        "Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1Guq_6-nUckv"
      },
      "outputs": [],
      "source": [
        "class TrainDataAugmentation(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TrainDataAugmentation, self).__init__()\n",
        "        self.transforms = v2.Compose([\n",
        "            v2.ToImage(),\n",
        "            # v2.RandomHorizontalFlip(p=0.5),\n",
        "            # v2.RandomAffine(degrees=(0, 30), translate=(0.2, 0.2)),\n",
        "            # v2.RandomRotation(degrees=(0, 180)),\n",
        "            # v2.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "            v2.ToDtype(torch.float32, scale=True),\n",
        "            v2.Normalize(mean=(-12.045004,), std=(17.43073,))\n",
        "        ])\n",
        "\n",
        "    def forward(self, image):\n",
        "        augmented_image = self.transforms(image)\n",
        "        return augmented_image\n",
        "\n",
        "class ValDataAugmentation(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ValDataAugmentation, self).__init__()\n",
        "        self.transforms = v2.Compose([\n",
        "            v2.ToImage(),\n",
        "            v2.ToDtype(torch.float32, scale=True),\n",
        "            v2.Normalize(mean=(-12.045004,), std=(17.43073,))\n",
        "        ])\n",
        "\n",
        "    def forward(self, image):\n",
        "        augmented_image = self.transforms(image)\n",
        "        return augmented_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4vv4yfYLlWP"
      },
      "source": [
        "Create Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mE2oqAvE3VYt"
      },
      "outputs": [],
      "source": [
        "class AudioDataset(Dataset):\n",
        "    def __init__(self, input_data, input_labels, transform):\n",
        "        self.input_data = input_data\n",
        "        self.input_labels = input_labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        song = self.input_data[idx]\n",
        "        genre = self.input_labels[idx]\n",
        "        if self.transform:\n",
        "            song = self.transform(song)\n",
        "\n",
        "        return song, genre\n",
        "\n",
        "audio_train_dataset = AudioDataset(input_data=train_data, input_labels=train_labels, transform=TrainDataAugmentation())\n",
        "audio_val_dataset = AudioDataset(input_data=val_data, input_labels=val_labels, transform=ValDataAugmentation())\n",
        "\n",
        "audio_train_loader = DataLoader(audio_train_dataset, batch_size=64, shuffle=True, pin_memory=True)\n",
        "audio_val_loader = DataLoader(audio_val_dataset, batch_size=64, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1z-eZvOeDvd5"
      },
      "outputs": [],
      "source": [
        "# dataiter = iter(audio_train_loader)\n",
        "# normalized_image_tensor = next(dataiter)[0]\n",
        "# normalized_image_array = normalized_image_tensor.numpy()[0]\n",
        "# normalized_image_array = np.transpose(normalized_image_array, (1, 2, 0))\n",
        "# plt.figure(figsize=(6, 6))\n",
        "# plt.title('After Normalization')\n",
        "# plt.imshow(normalized_image_array)\n",
        "# plt.colorbar()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbE0ZJlSAbkL"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbUesLLxhQ4E"
      },
      "source": [
        "Print Layer (debugging)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nWc2Rmz9hQNh"
      },
      "outputs": [],
      "source": [
        "class PrintLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PrintLayer, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Do your print / debug stuff here\n",
        "        print(\"X shape:\", x.shape)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b8PGs4MY0lZ"
      },
      "source": [
        "Positional Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Pvo_HoTlYzod"
      },
      "outputs": [],
      "source": [
        "# From: https://github.com/wzlxjtu/PositionalEncoding2D\n",
        "class PositionalEncoding1d(nn.Module):\n",
        "    def __init__(self, d_model, length):\n",
        "        super(PositionalEncoding1d, self).__init__()\n",
        "        if d_model % 2 != 0:\n",
        "            raise ValueError(\"Cannot use sin/cos positional encoding with \"\n",
        "                            \"odd dim (got dim={:d})\".format(d_model))\n",
        "        pe = torch.zeros(length, d_model)\n",
        "        position = torch.arange(0, length).unsqueeze(1)\n",
        "        div_term = torch.exp((torch.arange(0, d_model, 2, dtype=torch.float) *\n",
        "                            -(math.log(10000.0) / d_model)))\n",
        "        pe[:, 0::2] = torch.sin(position.float() * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position.float() * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe\n",
        "\n",
        "\n",
        "class PositionalEncoding2d(nn.Module):\n",
        "    def __init__(self, d_model, height, width):\n",
        "        super(PositionalEncoding2d, self).__init__()\n",
        "        if d_model % 4 != 0:\n",
        "            raise ValueError(\"Cannot use sin/cos positional encoding with \"\n",
        "                            \"odd dimension (got dim={:d})\".format(d_model))\n",
        "        pe = torch.zeros(d_model, height, width)\n",
        "        # Each dimension use half of d_model\n",
        "        d_model = int(d_model / 2)\n",
        "        div_term = torch.exp(torch.arange(0., d_model, 2) *\n",
        "                            -(math.log(10000.0) / d_model))\n",
        "        pos_w = torch.arange(0., width).unsqueeze(1)\n",
        "        pos_h = torch.arange(0., height).unsqueeze(1)\n",
        "        pe[0:d_model:2, :, :] = torch.sin(pos_w * div_term).transpose(0, 1).unsqueeze(1).repeat(1, height, 1)\n",
        "        pe[1:d_model:2, :, :] = torch.cos(pos_w * div_term).transpose(0, 1).unsqueeze(1).repeat(1, height, 1)\n",
        "        pe[d_model::2, :, :] = torch.sin(pos_h * div_term).transpose(0, 1).unsqueeze(2).repeat(1, 1, width)\n",
        "        pe[d_model + 1::2, :, :] = torch.cos(pos_h * div_term).transpose(0, 1).unsqueeze(2).repeat(1, 1, width)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iq58xQoFurNG"
      },
      "source": [
        "Separable Convolution 2D Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "AzNfP3uMuuA5"
      },
      "outputs": [],
      "source": [
        "# Inspired by: https://github.com/reshalfahsi/separableconv-torch\n",
        "class SeparableConv2d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding=1, bias=False):\n",
        "        super(SeparableConv2d, self).__init__()\n",
        "        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size, stride=stride, padding=padding, groups=in_channels, bias=bias)\n",
        "        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.depthwise(x)\n",
        "        x = self.pointwise(x)\n",
        "        return x\n",
        "\n",
        "class SeparableConv1d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding=1, bias=False):\n",
        "        super(SeparableConv1d, self).__init__()\n",
        "        self.depthwise = nn.Conv1d(in_channels, in_channels, kernel_size=kernel_size, stride=stride, padding=padding, groups=in_channels, bias=bias)\n",
        "        self.pointwise = nn.Conv1d(in_channels, out_channels, kernel_size=1, bias=bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.depthwise(x)\n",
        "        x = self.pointwise(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-nLapjftrVW"
      },
      "source": [
        "Residual Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "s3f_RdYZtqoS"
      },
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, num_layers: int, pool: bool, short: bool, two_dim: bool):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.pooling = pool\n",
        "        self.short = short\n",
        "        self.two_dim = two_dim\n",
        "\n",
        "        self.inconv = nn.Sequential(\n",
        "            SeparableConv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=1, padding=1, bias=False),\n",
        "            nn.SELU()\n",
        "        )\n",
        "\n",
        "        layers = []\n",
        "        for _ in range(num_layers - 1):\n",
        "            if self.two_dim:\n",
        "              layers.append(SeparableConv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=kernel_size, stride=1, padding=1, bias=False))\n",
        "            else:\n",
        "              layers.append(SeparableConv1d(in_channels=out_channels, out_channels=out_channels, kernel_size=kernel_size, stride=1, padding=1, bias=False))\n",
        "            layers.append(nn.SELU())\n",
        "        self.convlayers = nn.Sequential(*layers)\n",
        "\n",
        "        if self.pooling:\n",
        "            if self.two_dim:\n",
        "              self.pool = nn.MaxPool2d(kernel_size=kernel_size, stride=2, padding=1)\n",
        "              self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=2, bias=False)\n",
        "            else:\n",
        "              self.pool = nn.MaxPool1d(kernel_size=kernel_size, stride=2, padding=1)\n",
        "              self.shortcut = nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=2, bias=False)\n",
        "        else:\n",
        "            if self.two_dim:\n",
        "              self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
        "            else:\n",
        "              self.shortcut = nn.Conv1d(in_channels, out_channels, kernel_size=1, bias=False)\n",
        "\n",
        "        self.sact = nn.SELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.inconv(x)\n",
        "        out = self.convlayers(out)\n",
        "\n",
        "        if self.pooling:\n",
        "            out = self.pool(out)\n",
        "\n",
        "        if self.short:\n",
        "            shortcut = self.shortcut(x)\n",
        "            out = out + shortcut\n",
        "            out = self.sact(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SN5LxjCdKani"
      },
      "source": [
        "Embedding Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "6TKXXLVXfxHo"
      },
      "outputs": [],
      "source": [
        "class EmbeddingModel(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(EmbeddingModel, self).__init__()\n",
        "      self.input = nn.Sequential(nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "                                 nn.SELU())\n",
        "      self.positional_encoding = PositionalEncoding2d(64, 128, 130)\n",
        "\n",
        "\n",
        "      self.conv_layers = nn.Sequential(\n",
        "          ResidualBlock(in_channels=64, out_channels=64, kernel_size=3, num_layers=3, pool=True, short=True, two_dim=True),\n",
        "          ResidualBlock(in_channels=64, out_channels=128, kernel_size=3, num_layers=3, pool=True, short=True, two_dim=True),\n",
        "          ResidualBlock(in_channels=128, out_channels=256, kernel_size=3, num_layers=3, pool=True, short=True, two_dim=True),\n",
        "      )\n",
        "\n",
        "      self.attention = nn.MultiheadAttention(embed_dim=272, num_heads=16, dropout=0.5, batch_first=True)\n",
        "\n",
        "      self.dense_layers = nn.Sequential(\n",
        "        nn.Linear(in_features=256, out_features=1024, bias=False),\n",
        "        nn.SELU(),\n",
        "        nn.Linear(in_features=1024, out_features=512, bias=False),\n",
        "        nn.SELU(),\n",
        "        nn.Linear(in_features=512, out_features=256, bias=False),\n",
        "        nn.SELU(),\n",
        "        nn.Dropout(0.5),\n",
        "      )\n",
        "\n",
        "      self.output = nn.Linear(256, 8)\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = self.input(x)\n",
        "      # x = F.layer_norm(self.positional_encoding(x), x.shape)\n",
        "      x = self.conv_layers(x)\n",
        "\n",
        "      batch_size, channels, height, width = x.size()\n",
        "      x = x.view(batch_size, channels, height * width)\n",
        "      attention_output, _ = self.attention(x, x, x)\n",
        "      x = F.layer_norm(x + attention_output, x.shape)\n",
        "\n",
        "      x = torch.mean(x.view(x.size(0), x.size(1), -1), dim=2)\n",
        "      x = self.dense_layers(x)\n",
        "      out = self.output(x)\n",
        "      return out\n",
        "\n",
        "    def get_embedding(self, x):\n",
        "      return self.forward(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "GqNYS8CbqRvy"
      },
      "outputs": [],
      "source": [
        "# From https://github.com/adambielski/siamese-triplet\n",
        "class SimpleEmbeddingModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleEmbeddingModel, self).__init__()\n",
        "        self.convnet = nn.Sequential(nn.Conv2d(1, 32, 5),\n",
        "                                     nn.SELU(),\n",
        "                                     nn.MaxPool2d(2, stride=2),\n",
        "                                     nn.Conv2d(32, 64, 5),\n",
        "                                     nn.SELU(),\n",
        "                                     nn.MaxPool2d(2, stride=2),\n",
        "                                     nn.Conv2d(64, 128, 5),\n",
        "                                     nn.SELU(),\n",
        "                                     nn.MaxPool2d(2, stride=2)\n",
        "                                    )\n",
        "\n",
        "        self.fc = nn.Sequential(nn.Linear(2048, 256),\n",
        "                                nn.PReLU(),\n",
        "                                nn.Linear(256, 256),\n",
        "                                nn.PReLU(),\n",
        "                                nn.Linear(256, 10)\n",
        "                                )\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.convnet(x)\n",
        "        output = output.view(output.size()[0], -1)\n",
        "        output = self.fc(output)\n",
        "        return output\n",
        "\n",
        "    def get_embedding(self, x):\n",
        "        return self.forward(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvwY-DtHAgIk"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQdZH7di-4FI"
      },
      "source": [
        "Early Stopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "yHfW_2Qv-4-8"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=3, min_delta=0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.min_validation_loss = float('inf')\n",
        "        self.max_accuracy = float('-inf')\n",
        "\n",
        "    def early_stop(self, validation_loss):\n",
        "        if validation_loss < self.min_validation_loss:\n",
        "            self.min_validation_loss = validation_loss\n",
        "            self.counter = 0\n",
        "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def reset_loss(self):\n",
        "        self.min_validation_loss = float('inf')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQ9LNzJpANwb"
      },
      "source": [
        "Autoclip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "dq9wL52aANCt"
      },
      "outputs": [],
      "source": [
        "# From: https://github.com/pseeth/autoclip/blob/master/autoclip.py\n",
        "class AutoClip:\n",
        "    def __init__(self, percentile):\n",
        "        self.grad_history = []\n",
        "        self.percentile = percentile\n",
        "\n",
        "    def compute_grad_norm(self, model):\n",
        "        total_norm = 0\n",
        "        for p in model.parameters():\n",
        "            if p.grad is not None:\n",
        "                param_norm = p.grad.data.norm(2)\n",
        "                total_norm += param_norm.item() ** 2\n",
        "        total_norm = total_norm ** (1. / 2)\n",
        "\n",
        "        return total_norm\n",
        "\n",
        "    def __call__(self, model):\n",
        "        grad_norm = self.compute_grad_norm(model)\n",
        "        self.grad_history.append(grad_norm)\n",
        "        clip_value = np.percentile(self.grad_history, self.percentile)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkh_iRGFAkqF"
      },
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-c4IZbZAn0f",
        "outputId": "7df2bcc3-cdf1-4049-def6-f56d8447176b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1782/1782 [02:39<00:00, 11.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 1.620767 - Train Accuracy: 40.930135 - Val loss: 1.465287 - Val Accuracy: 48.379630 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1782/1782 [02:42<00:00, 11.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 1.445575 - Train Accuracy: 49.463384 - Val loss: 1.386471 - Val Accuracy: 51.206510 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1782/1782 [02:42<00:00, 10.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 1.373900 - Train Accuracy: 52.290264 - Val loss: 1.330489 - Val Accuracy: 53.682660 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1782/1782 [02:42<00:00, 11.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 1.324000 - Train Accuracy: 54.192971 - Val loss: 1.279162 - Val Accuracy: 55.843154 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1782/1782 [02:41<00:00, 11.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 1.265953 - Train Accuracy: 56.432379 - Val loss: 1.243408 - Val Accuracy: 57.330247 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1782/1782 [02:41<00:00, 11.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 1.221141 - Train Accuracy: 58.080808 - Val loss: 1.228262 - Val Accuracy: 57.737093 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1782/1782 [02:42<00:00, 11.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 1.195553 - Train Accuracy: 58.950617 - Val loss: 1.194518 - Val Accuracy: 58.789282 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1782/1782 [02:41<00:00, 11.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 1.158760 - Train Accuracy: 60.509610 - Val loss: 1.167688 - Val Accuracy: 59.687149 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1782/1782 [02:42<00:00, 10.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 1.135608 - Train Accuracy: 61.370651 - Val loss: 1.157007 - Val Accuracy: 60.514871 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1782/1782 [02:42<00:00, 10.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 1.120500 - Train Accuracy: 61.826599 - Val loss: 1.149520 - Val Accuracy: 60.718294 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1782/1782 [02:42<00:00, 10.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 1.215855 - Train Accuracy: 57.910704 - Val loss: 1.202957 - Val Accuracy: 58.438552 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1782/1782 [02:42<00:00, 10.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 1.180230 - Train Accuracy: 59.560887 - Val loss: 1.174834 - Val Accuracy: 59.385522 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1782/1782 [02:42<00:00, 10.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 1.147462 - Train Accuracy: 60.581510 - Val loss: 1.147388 - Val Accuracy: 60.676207 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1782/1782 [02:42<00:00, 10.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 1.103904 - Train Accuracy: 62.310606 - Val loss: 1.205548 - Val Accuracy: 58.557800 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1782/1782 [02:43<00:00, 10.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 1.066457 - Train Accuracy: 63.401375 - Val loss: 1.107104 - Val Accuracy: 62.149270 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1782/1782 [02:44<00:00, 10.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 1.010874 - Train Accuracy: 65.616232 - Val loss: 1.086445 - Val Accuracy: 62.668350 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1782/1782 [02:44<00:00, 10.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.994225 - Train Accuracy: 66.163370 - Val loss: 1.064101 - Val Accuracy: 63.566218 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1782/1782 [02:44<00:00, 10.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.947724 - Train Accuracy: 67.718855 - Val loss: 1.055245 - Val Accuracy: 63.902918 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1782/1782 [02:44<00:00, 10.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.921587 - Train Accuracy: 68.767536 - Val loss: 1.042028 - Val Accuracy: 64.597363 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1782/1782 [02:44<00:00, 10.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.906313 - Train Accuracy: 69.256804 - Val loss: 1.032207 - Val Accuracy: 65.088384 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1782/1782 [02:44<00:00, 10.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 1.031047 - Train Accuracy: 64.564043 - Val loss: 1.127216 - Val Accuracy: 61.251403 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1782/1782 [02:45<00:00, 10.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 1.006710 - Train Accuracy: 65.593434 - Val loss: 1.097923 - Val Accuracy: 62.464927 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1782/1782 [02:45<00:00, 10.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.959149 - Train Accuracy: 67.129630 - Val loss: 1.055013 - Val Accuracy: 64.064254 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1782/1782 [02:46<00:00, 10.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.935044 - Train Accuracy: 68.041526 - Val loss: 1.057285 - Val Accuracy: 64.092312 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1782/1782 [02:46<00:00, 10.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.878254 - Train Accuracy: 69.944234 - Val loss: 1.040333 - Val Accuracy: 64.913019 \n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1782/1782 [02:45<00:00, 10.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.833575 - Train Accuracy: 71.725940 - Val loss: 1.009445 - Val Accuracy: 66.119529 \n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1782/1782 [02:45<00:00, 10.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.790352 - Train Accuracy: 73.209526 - Val loss: 1.011281 - Val Accuracy: 66.575477 \n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1782/1782 [02:45<00:00, 10.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.743137 - Train Accuracy: 74.924593 - Val loss: 0.996140 - Val Accuracy: 67.010382 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1782/1782 [02:45<00:00, 10.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.712435 - Train Accuracy: 76.020623 - Val loss: 0.993000 - Val Accuracy: 67.501403 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1782/1782 [02:45<00:00, 10.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.693220 - Train Accuracy: 76.702792 - Val loss: 0.979741 - Val Accuracy: 68.062570 \n",
            "\n",
            "Epoch 31\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1782/1782 [02:46<00:00, 10.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.854272 - Train Accuracy: 70.792999 - Val loss: 1.034014 - Val Accuracy: 65.319865 \n",
            "\n",
            "Epoch 32\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1782/1782 [02:46<00:00, 10.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.830968 - Train Accuracy: 71.669823 - Val loss: 1.051595 - Val Accuracy: 64.765713 \n",
            "\n",
            "Epoch 33\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1782/1782 [02:46<00:00, 10.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.791388 - Train Accuracy: 72.899130 - Val loss: 1.046861 - Val Accuracy: 64.941077 \n",
            "\n",
            "Epoch 34\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1782/1782 [02:47<00:00, 10.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.718024 - Train Accuracy: 75.464717 - Val loss: 1.088680 - Val Accuracy: 64.997194 \n",
            "\n",
            "Epoch 35\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1782/1782 [02:47<00:00, 10.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.712201 - Train Accuracy: 75.706720 - Val loss: 1.010469 - Val Accuracy: 66.273850 \n",
            "\n",
            "Epoch 36\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1782/1782 [02:47<00:00, 10.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.634654 - Train Accuracy: 78.535354 - Val loss: 1.028356 - Val Accuracy: 67.522447 \n",
            "\n",
            "Epoch 37\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1782/1782 [02:48<00:00, 10.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.595338 - Train Accuracy: 79.876894 - Val loss: 1.010678 - Val Accuracy: 68.427329 \n",
            "\n",
            "Epoch 38\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1782/1782 [02:48<00:00, 10.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.545654 - Train Accuracy: 81.646324 - Val loss: 0.983801 - Val Accuracy: 69.416386 \n",
            "\n",
            "Epoch 39\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1782/1782 [02:47<00:00, 10.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.511204 - Train Accuracy: 82.861602 - Val loss: 1.001706 - Val Accuracy: 69.409371 \n",
            "\n",
            "Epoch 40\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1782/1782 [02:47<00:00, 10.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.488330 - Train Accuracy: 83.780513 - Val loss: 1.001884 - Val Accuracy: 69.542649 \n",
            "\n",
            "Epoch 41\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1782/1782 [02:47<00:00, 10.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.670222 - Train Accuracy: 76.890432 - Val loss: 1.028533 - Val Accuracy: 66.842031 \n",
            "\n",
            "Epoch 42\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1782/1782 [02:48<00:00, 10.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.662165 - Train Accuracy: 77.363917 - Val loss: 1.096492 - Val Accuracy: 65.137486 \n",
            "\n",
            "Epoch 43\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1782/1782 [02:48<00:00, 10.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.635550 - Train Accuracy: 78.310887 - Val loss: 1.052966 - Val Accuracy: 67.087542 \n",
            "\n",
            "Epoch 44\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1782/1782 [02:48<00:00, 10.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.576629 - Train Accuracy: 80.180275 - Val loss: 1.065001 - Val Accuracy: 66.694725 \n",
            "\n",
            "Epoch 45\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1782/1782 [02:49<00:00, 10.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.524415 - Train Accuracy: 82.084736 - Val loss: 1.014688 - Val Accuracy: 69.402357 \n",
            "\n",
            "Epoch 46\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1782/1782 [02:50<00:00, 10.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.468663 - Train Accuracy: 84.104938 - Val loss: 1.031692 - Val Accuracy: 69.121773 \n",
            "\n",
            "Epoch 47\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1782/1782 [02:50<00:00, 10.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.417185 - Train Accuracy: 85.919964 - Val loss: 1.092173 - Val Accuracy: 69.135802 \n",
            "\n",
            "Epoch 48\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1782/1782 [02:50<00:00, 10.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.368045 - Train Accuracy: 87.813903 - Val loss: 1.127281 - Val Accuracy: 69.163861 \n",
            "\n",
            "Epoch 49\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1782/1782 [02:51<00:00, 10.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.333399 - Train Accuracy: 88.929223 - Val loss: 1.121439 - Val Accuracy: 69.921437 \n",
            "\n",
            "Epoch 50\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1782/1782 [02:51<00:00, 10.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.311776 - Train Accuracy: 89.655233 - Val loss: 1.125502 - Val Accuracy: 70.209035 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "cuda = torch.cuda.is_available()\n",
        "embedding_model = EmbeddingModel().to(device)\n",
        "simple_embedding_model = SimpleEmbeddingModel().to(device)\n",
        "\n",
        "num_epochs = 50\n",
        "learning_rate = 1e-4\n",
        "classification_loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.AdamW(embedding_model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer=optimizer, T_0=10)\n",
        "early_stopping = EarlyStopping(patience=3)\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "autoclipper = AutoClip(percentile=10)\n",
        "\n",
        "def train_loop(train_loader, model, criterion, optimizer, device, epoch):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    num_batches = len(train_loader)\n",
        "\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    # Training loop\n",
        "    for batch, (inputs, targets) in enumerate(tqdm(train_loader)):\n",
        "        optimizer.zero_grad()\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "        with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
        "          outputs = model(inputs)\n",
        "          loss = criterion(outputs, targets)\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        scaler.scale(loss).backward()\n",
        "        autoclipper(model)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_correct += (predicted == targets).sum().item()\n",
        "        total_samples += targets.size(0)\n",
        "\n",
        "        scaler.step(optimizer)\n",
        "        scale = scaler.get_scale()\n",
        "        scaler.update()\n",
        "        skip_lr_sched = (scale != scaler.get_scale())\n",
        "\n",
        "    accuracy = 100 * total_correct / total_samples\n",
        "    train_loss = train_loss / num_batches\n",
        "    return train_loss, accuracy, skip_lr_sched\n",
        "\n",
        "def val_loop(val_loader, model, criterion, device, epoch):\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    num_batches = len(val_loader)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_correct += (predicted == targets).sum().item()\n",
        "            total_samples += targets.size(0)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    accuracy = 100 * total_correct / total_samples\n",
        "    val_loss /= num_batches\n",
        "    return val_loss, accuracy\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
        "    train_loss, train_accuracy, skip_lr_sched = train_loop(audio_train_loader, embedding_model, classification_loss_fn, optimizer, device, epoch+1)\n",
        "    val_loss, val_accuracy = val_loop(audio_val_loader, embedding_model, classification_loss_fn, device, epoch+1)\n",
        "    print(f\"Train loss: {train_loss:>8f} - Train Accuracy: {train_accuracy:>f} - Val loss: {val_loss:>8f} - Val Accuracy: {val_accuracy:>f} \\n\")\n",
        "\n",
        "    # early_stop_result = early_stopping.early_stop(val_loss)\n",
        "    # if early_stop_result:\n",
        "    #     print(f\"Early stopping after {epoch+1} epochs \\n\")\n",
        "    #     print(f\"Best val loss: {early_stopping.min_validation_loss} \\n\")\n",
        "    #     break\n",
        "\n",
        "    if not skip_lr_sched:\n",
        "        scheduler.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qw3nYWqqKkKw"
      },
      "outputs": [],
      "source": [
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}