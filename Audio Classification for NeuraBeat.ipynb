{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrtmFa8IAtps"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUn1Z--HIHSc",
        "outputId": "745919a8-80e5-4b41-ae1e-14dc21cdab39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import os\n",
        "import csv\n",
        "import string\n",
        "import librosa\n",
        "import shutil\n",
        "import torch\n",
        "import random\n",
        "import math\n",
        "import h5py\n",
        "import torchaudio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchaudio.transforms as T\n",
        "import torch.optim as optim\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.utils.data.sampler import BatchSampler\n",
        "from torch.utils.data import Subset\n",
        "from librosa.util import normalize\n",
        "from librosa.util import fix_length\n",
        "from google.colab import drive\n",
        "from google.colab import runtime\n",
        "from itertools import combinations\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
        "from torchvision import models, transforms\n",
        "from torchvision.transforms import v2\n",
        "from torchsummary import summary\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision import transforms\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrAl3KiDATum"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MNIST (Testing)"
      ],
      "metadata": {
        "id": "FHrmA-6ouybO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean, std = 0.1307, 0.3081\n",
        "\n",
        "# Define transforms\n",
        "transform = v2.Compose([\n",
        "    v2.Resize((64, 64)),\n",
        "    v2.ToImage(),\n",
        "    v2.ToDtype(torch.float32, scale=True),\n",
        "    v2.Normalize((mean,), (std,))\n",
        "])\n",
        "\n",
        "# Load datasets\n",
        "train_dataset = MNIST('../data/MNIST', train=True, download=True, transform=transform)\n",
        "test_dataset = MNIST('../data/MNIST', train=False, download=True, transform=transform)\n",
        "\n",
        "# Define the target number of samples\n",
        "num_train_samples = 6400\n",
        "num_val_samples = 1600\n",
        "\n",
        "# Shuffle and select a subset of the training dataset\n",
        "train_indices = np.random.permutation(len(train_dataset))[:num_train_samples]\n",
        "train_subset = Subset(train_dataset, train_indices)\n",
        "\n",
        "# Shuffle and select a subset of the validation dataset\n",
        "val_indices = np.random.permutation(len(test_dataset))[:num_val_samples]\n",
        "val_subset = Subset(test_dataset, val_indices)\n",
        "\n",
        "# Create DataLoaders for the subsets\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else {}\n",
        "train_loader = DataLoader(train_subset, shuffle=True, batch_size=256, **kwargs)\n",
        "val_loader = DataLoader(val_subset, batch_size=256, **kwargs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWGQaxV6uuit",
        "outputId": "69beacee-510f-424e-b017-894ee21267cb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 50873754.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 1746125.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 4623620.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 12087898.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-mIqsO2IHSi"
      },
      "source": [
        "Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DpPQXzhIl0tO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29fff871-8eb5-42f0-9d56-98cf465acead"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Electronic': 0, 'Experimental': 1, 'Folk': 2, 'Hip-Hop': 3, 'Instrumental': 4, 'International': 5, 'Pop': 6, 'Rock': 7}\n"
          ]
        }
      ],
      "source": [
        "# Load training data from HDF5 file\n",
        "with h5py.File('/content/drive/My Drive/Projects/NeuraBeat/Data/train_data_melspec_expanded.h5', 'r') as train_file:\n",
        "    train_data = np.array(train_file['data'])\n",
        "    train_labels = np.array(train_file['labels'])\n",
        "\n",
        "# Load validation data from HDF5 file\n",
        "with h5py.File('/content/drive/My Drive/Projects/NeuraBeat/Data/val_data_melspec_expanded.h5', 'r') as val_file:\n",
        "    val_data = np.array(val_file['data'])\n",
        "    val_labels = np.array(val_file['labels'])\n",
        "\n",
        "train_data = np.transpose(train_data, (0, 2, 3, 1))\n",
        "val_data = np.transpose(val_data, (0, 2, 3, 1))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to plot samples for each label in a row\n",
        "def plot_samples_in_row(data, labels, num_samples=5):\n",
        "    unique_labels = torch.unique(labels)\n",
        "    num_labels = len(unique_labels)\n",
        "    num_cols = num_samples\n",
        "    num_rows = math.ceil(num_labels / num_cols)\n",
        "\n",
        "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 5*num_rows))\n",
        "\n",
        "    for i, label in enumerate(unique_labels):\n",
        "        label_indices = (labels == label).nonzero(as_tuple=True)[0][:num_samples]\n",
        "        for j, idx in enumerate(label_indices):\n",
        "            ax = axes[i // num_cols, j % num_cols]\n",
        "            ax.imshow(data[idx])\n",
        "            ax.set_title(f\"Label: {label.item()}\")\n",
        "            ax.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "z-EzqmMyLoSa"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mean_std(data):\n",
        "  mean = np.mean(data, axis=(0, 1, 2))\n",
        "  std = np.std(data, axis=(0, 1, 2))\n",
        "  return mean, std\n",
        "\n",
        "print(get_mean_std(train_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAhb0BTj4ovl",
        "outputId": "2ac15339-3d4a-46c4-d7ac-a9bcb72343aa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(array([-39.2564], dtype=float32), array([12.466536], dtype=float32))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Augmentation"
      ],
      "metadata": {
        "id": "58erhwFnGr0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainDataAugmentation(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TrainDataAugmentation, self).__init__()\n",
        "        self.transforms = v2.Compose([\n",
        "            v2.ToImage(),\n",
        "            # v2.RandomHorizontalFlip(p=0.5),\n",
        "            # v2.RandomAffine(degrees=(0, 30), translate=(0.2, 0.2)),\n",
        "            # v2.RandomRotation(degrees=(0, 180)),\n",
        "            # v2.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "            v2.ToDtype(torch.float32, scale=True),\n",
        "            v2.Normalize(mean=(-39.2564,), std=(12.466536,))\n",
        "        ])\n",
        "\n",
        "    def forward(self, image):\n",
        "        augmented_image = self.transforms(image)\n",
        "        return augmented_image\n",
        "\n",
        "class ValDataAugmentation(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ValDataAugmentation, self).__init__()\n",
        "        self.transforms = v2.Compose([\n",
        "            v2.ToImage(),\n",
        "            v2.ToDtype(torch.float32, scale=True),\n",
        "            v2.Normalize(mean=(-36.098557,), std=(13.916812,))\n",
        "        ])\n",
        "\n",
        "    def forward(self, image):\n",
        "        augmented_image = self.transforms(image)\n",
        "        return augmented_image"
      ],
      "metadata": {
        "id": "1Guq_6-nUckv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4vv4yfYLlWP"
      },
      "source": [
        "Create Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mE2oqAvE3VYt"
      },
      "outputs": [],
      "source": [
        "class AudioDataset(Dataset):\n",
        "    def __init__(self, input_data, input_labels, transform):\n",
        "        self.input_data = input_data\n",
        "        self.input_labels = input_labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        song = self.input_data[idx]\n",
        "        genre = self.input_labels[idx]\n",
        "        if self.transform:\n",
        "            song = self.transform(song)\n",
        "\n",
        "        return song, genre\n",
        "\n",
        "audio_train_dataset = AudioDataset(input_data=train_data, input_labels=train_labels, transform=TrainDataAugmentation())\n",
        "audio_val_dataset = AudioDataset(input_data=val_data, input_labels=val_labels, transform=ValDataAugmentation())\n",
        "\n",
        "audio_train_loader = DataLoader(audio_train_dataset, batch_size=128, shuffle=True, pin_memory=True)\n",
        "audio_val_loader = DataLoader(audio_val_dataset, batch_size=128, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbE0ZJlSAbkL"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbUesLLxhQ4E"
      },
      "source": [
        "Print Layer (debugging)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nWc2Rmz9hQNh"
      },
      "outputs": [],
      "source": [
        "class PrintLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PrintLayer, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Do your print / debug stuff here\n",
        "        print(\"X shape:\", x.shape)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positional Encoding"
      ],
      "metadata": {
        "id": "0b8PGs4MY0lZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# From: https://github.com/wzlxjtu/PositionalEncoding2D\n",
        "class PositionalEncoding1d(nn.Module):\n",
        "    def __init__(self, d_model, length):\n",
        "        super(PositionalEncoding1d, self).__init__()\n",
        "        if d_model % 2 != 0:\n",
        "            raise ValueError(\"Cannot use sin/cos positional encoding with \"\n",
        "                            \"odd dim (got dim={:d})\".format(d_model))\n",
        "        pe = torch.zeros(length, d_model)\n",
        "        position = torch.arange(0, length).unsqueeze(1)\n",
        "        div_term = torch.exp((torch.arange(0, d_model, 2, dtype=torch.float) *\n",
        "                            -(math.log(10000.0) / d_model)))\n",
        "        pe[:, 0::2] = torch.sin(position.float() * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position.float() * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe\n",
        "\n",
        "\n",
        "class PositionalEncoding2d(nn.Module):\n",
        "    def __init__(self, d_model, height, width):\n",
        "        super(PositionalEncoding2d, self).__init__()\n",
        "        if d_model % 4 != 0:\n",
        "            raise ValueError(\"Cannot use sin/cos positional encoding with \"\n",
        "                            \"odd dimension (got dim={:d})\".format(d_model))\n",
        "        pe = torch.zeros(d_model, height, width)\n",
        "        # Each dimension use half of d_model\n",
        "        d_model = int(d_model / 2)\n",
        "        div_term = torch.exp(torch.arange(0., d_model, 2) *\n",
        "                            -(math.log(10000.0) / d_model))\n",
        "        pos_w = torch.arange(0., width).unsqueeze(1)\n",
        "        pos_h = torch.arange(0., height).unsqueeze(1)\n",
        "        pe[0:d_model:2, :, :] = torch.sin(pos_w * div_term).transpose(0, 1).unsqueeze(1).repeat(1, height, 1)\n",
        "        pe[1:d_model:2, :, :] = torch.cos(pos_w * div_term).transpose(0, 1).unsqueeze(1).repeat(1, height, 1)\n",
        "        pe[d_model::2, :, :] = torch.sin(pos_h * div_term).transpose(0, 1).unsqueeze(2).repeat(1, 1, width)\n",
        "        pe[d_model + 1::2, :, :] = torch.cos(pos_h * div_term).transpose(0, 1).unsqueeze(2).repeat(1, 1, width)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe"
      ],
      "metadata": {
        "id": "Pvo_HoTlYzod"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Separable Convolution 2D Layer"
      ],
      "metadata": {
        "id": "iq58xQoFurNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspired by: https://github.com/reshalfahsi/separableconv-torch\n",
        "class SeparableConv2d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding=1, bias=False):\n",
        "        super(SeparableConv2d, self).__init__()\n",
        "        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size, stride=stride, padding=padding, groups=in_channels, bias=bias)\n",
        "        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.depthwise(x)\n",
        "        x = self.pointwise(x)\n",
        "        return x\n",
        "\n",
        "class SeparableConv1d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding=1, bias=False):\n",
        "        super(SeparableConv1d, self).__init__()\n",
        "        self.depthwise = nn.Conv1d(in_channels, in_channels, kernel_size=kernel_size, stride=stride, padding=padding, groups=in_channels, bias=bias)\n",
        "        self.pointwise = nn.Conv1d(in_channels, out_channels, kernel_size=1, bias=bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.depthwise(x)\n",
        "        x = self.pointwise(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "AzNfP3uMuuA5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Residual Block"
      ],
      "metadata": {
        "id": "H-nLapjftrVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, num_layers: int, pool: bool, short: bool, two_dim: bool):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.pooling = pool\n",
        "        self.short = short\n",
        "        self.two_dim = two_dim\n",
        "\n",
        "        self.inconv = nn.Sequential(\n",
        "            SeparableConv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=1, padding=1, bias=False),\n",
        "            nn.SELU()\n",
        "        )\n",
        "\n",
        "        layers = []\n",
        "        for _ in range(num_layers - 1):\n",
        "            if self.two_dim:\n",
        "              layers.append(SeparableConv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=kernel_size, stride=1, padding=1, bias=False))\n",
        "            else:\n",
        "              layers.append(SeparableConv1d(in_channels=out_channels, out_channels=out_channels, kernel_size=kernel_size, stride=1, padding=1, bias=False))\n",
        "            layers.append(nn.SELU())\n",
        "        self.convlayers = nn.Sequential(*layers)\n",
        "\n",
        "        if self.pooling:\n",
        "            if self.two_dim:\n",
        "              self.pool = nn.MaxPool2d(kernel_size=kernel_size, stride=2, padding=1)\n",
        "              self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=2, bias=False)\n",
        "            else:\n",
        "              self.pool = nn.MaxPool1d(kernel_size=kernel_size, stride=2, padding=1)\n",
        "              self.shortcut = nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=2, bias=False)\n",
        "        else:\n",
        "            if self.two_dim:\n",
        "              self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
        "            else:\n",
        "              self.shortcut = nn.Conv1d(in_channels, out_channels, kernel_size=1, bias=False)\n",
        "\n",
        "        self.sact = nn.SELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.inconv(x)\n",
        "        out = self.convlayers(out)\n",
        "\n",
        "        if self.pooling:\n",
        "            out = self.pool(out)\n",
        "\n",
        "        if self.short:\n",
        "            shortcut = self.shortcut(x)\n",
        "            out = out + shortcut\n",
        "            out = self.sact(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "s3f_RdYZtqoS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SN5LxjCdKani"
      },
      "source": [
        "Embedding Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "6TKXXLVXfxHo"
      },
      "outputs": [],
      "source": [
        "class EmbeddingModel(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(EmbeddingModel, self).__init__()\n",
        "      self.input = nn.Sequential(nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "                                 nn.SELU())\n",
        "      self.positional_encoding = PositionalEncoding2d(64, 128, 128)\n",
        "\n",
        "\n",
        "      self.conv_layers = nn.Sequential(\n",
        "            SeparableConv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.SELU(),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            SeparableConv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.SELU(),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            SeparableConv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.SELU(),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            SeparableConv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.SELU(),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "      )\n",
        "\n",
        "      self.attention = nn.MultiheadAttention(embed_dim=64, num_heads=4, dropout=0.5, batch_first=True)\n",
        "\n",
        "      self.dense_layers = nn.Sequential(\n",
        "        nn.Linear(in_features=512, out_features=1024, bias=False),\n",
        "        nn.SELU(),\n",
        "        nn.Linear(in_features=1024, out_features=512, bias=False),\n",
        "        nn.SELU(),\n",
        "        nn.Linear(in_features=512, out_features=256, bias=False),\n",
        "        nn.SELU(),\n",
        "        nn.Dropout(0.5),\n",
        "      )\n",
        "\n",
        "      self.output = nn.Linear(256, 8)\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = self.input(x)\n",
        "      # x = F.layer_norm(self.positional_encoding(x), x.shape)\n",
        "      x = self.conv_layers(x)\n",
        "\n",
        "      # batch_size, channels, height, width = x.size()\n",
        "      # x = x.view(batch_size, channels, height * width)\n",
        "      # attention_output, _ = self.attention(x, x, x)\n",
        "      # x = F.layer_norm(x + attention_output, x.shape)\n",
        "\n",
        "      x = torch.mean(x.view(x.size(0), x.size(1), -1), dim=2)\n",
        "      x = self.dense_layers(x)\n",
        "      out = self.output(x)\n",
        "      return out\n",
        "\n",
        "    def get_embedding(self, x):\n",
        "      return self.forward(x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# From https://github.com/adambielski/siamese-triplet\n",
        "class SimpleEmbeddingModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleEmbeddingModel, self).__init__()\n",
        "        self.convnet = nn.Sequential(nn.Conv2d(1, 32, 5),\n",
        "                                     nn.SELU(),\n",
        "                                     nn.MaxPool2d(2, stride=2),\n",
        "                                     nn.Conv2d(32, 64, 5),\n",
        "                                     nn.SELU(),\n",
        "                                     nn.MaxPool2d(2, stride=2),\n",
        "                                     nn.Conv2d(64, 128, 5),\n",
        "                                     nn.SELU(),\n",
        "                                     nn.MaxPool2d(2, stride=2)\n",
        "                                    )\n",
        "\n",
        "        self.fc = nn.Sequential(nn.Linear(2048, 256),\n",
        "                                nn.PReLU(),\n",
        "                                nn.Linear(256, 256),\n",
        "                                nn.PReLU(),\n",
        "                                nn.Linear(256, 10)\n",
        "                                )\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.convnet(x)\n",
        "        output = output.view(output.size()[0], -1)\n",
        "        output = self.fc(output)\n",
        "        return output\n",
        "\n",
        "    def get_embedding(self, x):\n",
        "        return self.forward(x)"
      ],
      "metadata": {
        "id": "GqNYS8CbqRvy"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvwY-DtHAgIk"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQdZH7di-4FI"
      },
      "source": [
        "Early Stopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "yHfW_2Qv-4-8"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=3, min_delta=0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.min_validation_loss = float('inf')\n",
        "        self.max_accuracy = float('-inf')\n",
        "\n",
        "    def early_stop(self, validation_loss):\n",
        "        if validation_loss < self.min_validation_loss:\n",
        "            self.min_validation_loss = validation_loss\n",
        "            self.counter = 0\n",
        "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def reset_loss(self):\n",
        "        self.min_validation_loss = float('inf')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQ9LNzJpANwb"
      },
      "source": [
        "Autoclip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "dq9wL52aANCt"
      },
      "outputs": [],
      "source": [
        "# From: https://github.com/pseeth/autoclip/blob/master/autoclip.py\n",
        "class AutoClip:\n",
        "    def __init__(self, percentile):\n",
        "        self.grad_history = []\n",
        "        self.percentile = percentile\n",
        "\n",
        "    def compute_grad_norm(self, model):\n",
        "        total_norm = 0\n",
        "        for p in model.parameters():\n",
        "            if p.grad is not None:\n",
        "                param_norm = p.grad.data.norm(2)\n",
        "                total_norm += param_norm.item() ** 2\n",
        "        total_norm = total_norm ** (1. / 2)\n",
        "\n",
        "        return total_norm\n",
        "\n",
        "    def __call__(self, model):\n",
        "        grad_norm = self.compute_grad_norm(model)\n",
        "        self.grad_history.append(grad_norm)\n",
        "        clip_value = np.percentile(self.grad_history, self.percentile)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkh_iRGFAkqF"
      },
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-c4IZbZAn0f"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "cuda = torch.cuda.is_available()\n",
        "embedding_model = EmbeddingModel().to(device)\n",
        "simple_embedding_model = SimpleEmbeddingModel().to(device)\n",
        "\n",
        "num_epochs = 50\n",
        "learning_rate = 1e-5\n",
        "classification_loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.Adam(embedding_model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "scheduler = lr_scheduler.StepLR(optimizer=optimizer, step_size=10)\n",
        "early_stopping = EarlyStopping(patience=3)\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "autoclipper = AutoClip(percentile=10)\n",
        "\n",
        "def train_loop(train_loader, model, criterion, optimizer, device, epoch):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    num_batches = len(train_loader)\n",
        "\n",
        "    # Training loop\n",
        "    for batch, (inputs, targets) in enumerate(tqdm(train_loader)):\n",
        "        optimizer.zero_grad()\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "        with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
        "          outputs = model(inputs)\n",
        "          loss = criterion(outputs, targets)\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        scaler.scale(loss).backward()\n",
        "        autoclipper(model)\n",
        "\n",
        "        scaler.step(optimizer)\n",
        "        scale = scaler.get_scale()\n",
        "        scaler.update()\n",
        "        skip_lr_sched = (scale != scaler.get_scale())\n",
        "\n",
        "    train_loss = train_loss / num_batches\n",
        "    return train_loss, skip_lr_sched\n",
        "\n",
        "def val_loop(val_loader, model, criterion, device, epoch):\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    num_batches = len(val_loader)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_correct += (predicted == targets).sum().item()\n",
        "            total_samples += targets.size(0)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    accuracy = 100 * total_correct / total_samples\n",
        "    val_loss /= num_batches\n",
        "    return val_loss, accuracy\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
        "    train_loss, skip_lr_sched = train_loop(audio_train_loader, embedding_model, classification_loss_fn, optimizer, device, epoch+1)\n",
        "    val_loss, accuracy = val_loop(audio_val_loader, embedding_model, classification_loss_fn, device, epoch+1)\n",
        "    print(f\"Train loss: {train_loss:>8f} - Val loss: {val_loss:>8f} - Accuracy: {accuracy:>f} \\n\")\n",
        "\n",
        "    # early_stop_result = early_stopping.early_stop(val_loss)\n",
        "    # if early_stop_result:\n",
        "    #     print(f\"Early stopping after {epoch+1} epochs \\n\")\n",
        "    #     print(f\"Best val loss: {early_stopping.min_validation_loss} \\n\")\n",
        "    #     break\n",
        "\n",
        "    if not skip_lr_sched:\n",
        "        scheduler.step()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "Qw3nYWqqKkKw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}